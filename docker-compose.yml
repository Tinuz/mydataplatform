services:
  postgres:
    image: postgres:16-alpine
    container_name: dp_postgres
    environment:
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
      POSTGRES_DB: superset
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U superset" ]
      interval: 5s
      timeout: 5s
      retries: 10
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./postgres-init:/docker-entrypoint-initdb.d
    deploy:
      resources:
        limits:
          memory: 1g
    profiles: [ "tiny", "standard" ]

  superset:
    build:
      context: ./superset
      dockerfile: Dockerfile
    image: dp_superset:latest
    container_name: dp_superset
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      SUPERSET_SECRET_KEY: "please_change_me"
      SUPERSET_ENV: production
      SUPERSET_LOAD_EXAMPLES: "no"
      DATABASE_DB: superset
      DATABASE_USER: superset
      DATABASE_PASSWORD: superset
      DATABASE_HOST: postgres
      DATABASE_PORT: 5432
      SUPERSET_DATABASE_URI: postgresql+psycopg2://superset:superset@postgres:5432/superset
      WTF_CSRF_ENABLED: "False" # alleen voor lokale dev!
      ENABLE_CORS: "True"
    ports:
      - "8088:8088"
    volumes:
      - ./superset/superset_bootstrap.sh:/docker-entrypoint-initdb.d/superset_bootstrap.sh
    deploy:
      resources:
        limits:
          memory: 1.5g
    profiles: [ "tiny", "standard" ]

  minio:
    image: minio/minio:latest
    container_name: dp_minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio12345
    ports:
      - "9000:9000" # S3 API
      - "9001:9001" # Console
    volumes:
      - minio_data:/data
    deploy:
      resources:
        limits:
          memory: 1g
    profiles: [ "standard", "streaming" ]  # Ook beschikbaar voor streaming

  dagster:
    build:
      context: ./orchestration
      dockerfile: Dockerfile
    image: dp_dagster:latest
    container_name: dp_dagster
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_started
    environment:
      DAGSTER_HOME: /opt/dagster/app/dagster_home
      MINIO_ENDPOINT: "minio:9000"
      MINIO_ACCESS_KEY: minio
      MINIO_SECRET_KEY: minio12345
      MINIO_BUCKET: lake
      POSTGRES_CONN: "postgresql://superset:superset@postgres:5432/superset"
      # AWS region for Iceberg/S3 SDK (required even for MinIO)
      AWS_REGION: us-east-1
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio12345
      # Google Cloud Storage
      GCS_BUCKET_NAME: public_data_demo
      GCS_CREDENTIALS_PATH: /opt/dagster/gcs-credentials.json
      GCS_PREFIX: investigations/
    ports:
      - "3000:3000"  # Dagster UI
      - "8011:8011"  # dbt docs
    volumes:
      - ./orchestration:/opt/dagster/app
      - ./dbt_investigations:/opt/dagster/dbt_investigations
      - ./gcs-credentials.json:/opt/dagster/gcs-credentials.json:ro
    deploy:
      resources:
        limits:
          memory: 2g  # Increased from 1g for crypto_stream with pyiceberg + kafka-python
    profiles: [ "standard" ]

  meltano:
    image: meltano/meltano:latest
    container_name: dp_meltano
    working_dir: /project
    command: [ "sleep", "infinity" ]
    volumes:
      - ./meltano:/project
    profiles: [ "standard" ]

  marquez:
    platform: linux/arm64
    image: marquezproject/marquez:0.51.1
    # op M2: gebruik ARM of laat platform weg (Docker kiest zelf ARM)
    container_name: dp_marquez
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # DB settings (zorg dat de DB 'marquez' bestaat)
      MARQUEZ_DB_HOST: postgres
      MARQUEZ_DB_PORT: 5432
      MARQUEZ_DB_NAME: marquez
      MARQUEZ_DB_USER: superset
      MARQUEZ_DB_PASSWORD: superset
      # API poort
      MARQUEZ_PORT: 5000
      # (optioneel) namespace die jij in ETL gebruikt
      MARQUEZ_NAMESPACE: demo
      # Als je config.yml mount, mag dat â€” maar dan hoeft MARQUEZ_URL niet
      MARQUEZ_CONFIG: /app/config.yml
    # Als je config mount: uncomment de volgende regel en zorg dat pad klopt
    volumes:
      - ./marquez/marquez-config.yml:/app/config.yml:ro
    command: [ "./docker/wait-for-it.sh", "postgres:5432", "--", "./entrypoint.sh" ]
    ports:
      - "5000:5000" # alleen de API!
    profiles: [ "standard" ]

  marquez-web:
    platform: linux/arm64
    image: marquezproject/marquez-web:0.51.1
    environment:
      MARQUEZ_HOST: marquez # service-naam van de API
      MARQUEZ_PORT: "5000" # interne API-poort
      MARQUEZ_PATH: "" # laat leeg
      MARQUEZ_TIMEOUT: "5000"
      # Web server settings (vereist):
      WEB_HOST: "0.0.0.0"
      WEB_PORT: "3000"
    ports:
      - "3001:3000" # UI op http://localhost:3001
    depends_on:
      - marquez
    profiles: [ "standard" ]

  etl:
    build:
      context: ./etl
      dockerfile: Dockerfile
    container_name: dp_etl
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_started
      superset:
        condition: service_started
      marquez:
        condition: service_started
    environment:
      # --- Bron (PUBLIC) ---
      FILE_URL: "https://storage.googleapis.com/cell_tower_data/204.csv"
      RAW_FILENAME: "204.csv" # optioneel; standaard = basename(FILE_URL)

      # --- MinIO (S3) ---
      MINIO_ENDPOINT: "http://minio:9000"
      MINIO_ACCESS_KEY: "minio"
      MINIO_SECRET_KEY: "minio12345"
      MINIO_BUCKET: "lake"
      MINIO_RAW_PREFIX: "raw/celltowers/"
      MINIO_CLEAN_PREFIX: "clean/celltowers/" # (nu niet gebruikt; gereserveerd voor evt. export)

      # --- Postgres (warehouse) ---
      PGHOST: "postgres"
      PGPORT: "5432"
      PGDATABASE: "superset"
      PGUSER: "superset"
      PGPASSWORD: "superset"

      # --- Superset API (catalogus) ---
      SUPERSET_URL: "http://superset:8088"
      SUPERSET_USER: "admin"
      SUPERSET_PASSWORD: "admin"

      # --- Marquez (lineage/catalog) ---
      MARQUEZ_URL: "http://marquez:5000"
      MARQUEZ_NAMESPACE: "demo"
      MARQUEZ_SOURCE_MINIO: "minio-lake"
      MARQUEZ_SOURCE_PG: "postgres-warehouse"
      MARQUEZ_JOB_NAME: "cell_towers_etl_v1"

      # --- Pipeline parameters ---
      SCHEMA_NAME: "cell_towers"
      STAGING_TABLE: "stg_204"
      CLEAN_TABLE: "clean_204"
    volumes:
      - ./etl/sql:/app/sql:ro
    profiles: [ "standard" ]

  cell-api:
    build:
      context: ./api
    container_name: dp_cell_api
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      PGHOST: postgres
      PGPORT: 5432
      PGDATABASE: superset
      PGUSER: superset
      PGPASSWORD: superset
      API_PORT: 3000
    ports:
      - "3100:3000" # optioneel voor directe debugging
    profiles: [ "standard" ]

  postgres_konga:
    image: postgres:9.6-alpine
    container_name: dp_postgres_konga
    environment:
      POSTGRES_USER: konga
      POSTGRES_PASSWORD: konga
      POSTGRES_DB: konga
    volumes:
      - konga_pgdata:/var/lib/postgresql/data
    ports:
      - "5433:5432"  # optioneel, alleen handig voor lokale psql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U konga -d konga || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 10
    deploy:
      resources:
        limits:
          memory: 512m
    profiles: [ "standard" ]

  kong:
    image: kong:3.6
    container_name: dp_kong
    environment:
      KONG_DATABASE: "off"  # DB-less mode - eenvoudig voor demo/development
      KONG_DECLARATIVE_CONFIG: /kong/declarative/kong.yml
      KONG_PROXY_LISTEN: "0.0.0.0:8000"
      KONG_ADMIN_LISTEN: "0.0.0.0:8001"
      KONG_LOG_LEVEL: notice
      # Extra security
      KONG_REAL_IP_HEADER: "X-Forwarded-For"
      KONG_REAL_IP_RECURSIVE: "on"
    volumes:
      - ./kong/kong.yml:/kong/declarative/kong.yml:ro
    ports:
      - "8000:8000"  # Proxy poort (API gateway)
      - "8001:8001"  # Admin API poort
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 512m
    profiles: [ "standard" ]

  konga:
    image: pantsel/konga:latest
    container_name: dp_konga
    platform: linux/amd64
    depends_on:
      postgres_konga:
        condition: service_healthy
      kong:
        condition: service_healthy
    environment:
      NODE_ENV: production
      # BELANGRIJK: Verander dit naar een sterke random string in productie!
      # Genereer met: openssl rand -base64 32
      TOKEN_SECRET: "jKz9mP2nQ5wX8vB3yL7tF6hR4dS1gA0e"
      
      # Database configuratie
      DB_ADAPTER: "postgres"
      DB_HOST: "postgres_konga"
      DB_PORT: "5432"
      DB_USER: "konga"
      DB_PASSWORD: "konga"
      DB_DATABASE: "konga"
      
      # Auto-migrate zorgt dat Konga de database schema's automatisch aanmaakt
      KONGA_AUTO_MIGRATE: "true"
      
      # Optioneel: seed data voor eerste admin user (handig voor automation)
      # KONGA_SEED_USER_DATA_SOURCE_FILE: "/app/userdb.data"
    volumes:
      - konga_data:/app/kongadata
    ports:
      - "1337:1337"  # Konga UI
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512m
    profiles: [ "standard" ]

  swagger-ui:
    image: swaggerapi/swagger-ui:latest
    environment:
      SWAGGER_JSON: /usr/share/nginx/html/openapi.yaml
    volumes:
      - ./api/openapi.yaml:/usr/share/nginx/html/openapi.yaml:ro
    ports:
      - "8082:8080"  # Changed from 8081 to avoid conflict with DataHub
    depends_on:
      - cell-api
    profiles: [ "standard" ]

  # Amundsen Services
  amundsen-neo4j:
    image: neo4j:4.4.30
    container_name: dp_amundsen_neo4j
    platform: linux/amd64
    environment:
      NEO4J_AUTH: neo4j/test
      NEO4J_dbms_memory_heap_max__size: 512m
    volumes:
      - amundsen_neo4j:/data
    ports:
      - "7474:7474"  # Neo4j Browser
      - "7687:7687"  # Bolt protocol
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "test", "RETURN 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: [ "amundsen" ]

  amundsen-elasticsearch:
    image: elasticsearch:8.10.2
    container_name: dp_amundsen_elasticsearch
    platform: linux/amd64
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms256m -Xmx512m
    volumes:
      - amundsen_elasticsearch:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: [ "amundsen" ]

  amundsen-metadata:
    image: amundsendev/amundsen-metadata:latest
    container_name: dp_amundsen_metadata
    platform: linux/amd64
    depends_on:
      amundsen-neo4j:
        condition: service_healthy
    environment:
      PROXY_HOST: bolt://amundsen-neo4j
      PROXY_PORT: 7687
      PROXY_USER: neo4j
      PROXY_PASSWORD: test
      PROXY_ENCRYPTED: "False"
    ports:
      - "5002:5000"  # Metadata service API
    profiles: [ "amundsen" ]

  amundsen-search:
    image: amundsendev/amundsen-search:latest
    container_name: dp_amundsen_search
    platform: linux/amd64
    depends_on:
      amundsen-elasticsearch:
        condition: service_healthy
    environment:
      PROXY_ENDPOINT: http://amundsen-elasticsearch:9200
    ports:
      - "5001:5000"  # Search service API
    profiles: [ "amundsen" ]

  amundsen-frontend:
    image: amundsendev/amundsen-frontend:latest
    container_name: dp_amundsen_frontend
    platform: linux/amd64
    depends_on:
      - amundsen-metadata
      - amundsen-search
    environment:
      SEARCHSERVICE_BASE: http://amundsen-search:5001
      METADATASERVICE_BASE: http://amundsen-metadata:5002
      FRONTEND_SVC_CONFIG_MODULE_CLASS: amundsen_application.config.LocalConfig
    ports:
      - "5005:5000"  # Amundsen UI (changed to avoid conflict with Marquez)
    profiles: [ "amundsen" ]

  # ==========================================
  # STREAMING + LAKEHOUSE (Kafka + Iceberg)
  # ==========================================

  kafka:
    image: apache/kafka:3.7.0
    container_name: dp_kafka
    platform: linux/arm64
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      # KRaft mode (no Zookeeper needed!)
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    volumes:
      - kafka_data:/var/lib/kafka/data
    deploy:
      resources:
        limits:
          memory: 1g
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions.sh --bootstrap-server localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5
    profiles: [ "streaming" ]

  iceberg-rest:
    image: tabulario/iceberg-rest:0.6.0
    container_name: dp_iceberg_rest
    ports:
      - "8181:8181"
    environment:
      # Catalog configuration
      CATALOG_WAREHOUSE: s3://lake/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: "true"
      CATALOG_S3_ACCESS__KEY__ID: minio
      CATALOG_S3_SECRET__ACCESS__KEY: minio12345
      # AWS region (required by AWS SDK even for MinIO)
      AWS_REGION: us-east-1
      # Metadata storage (SQLite for simplicity)
      CATALOG_JDBC_USER: admin
      CATALOG_JDBC_PASSWORD: password
    volumes:
      - iceberg_catalog_data:/app
    depends_on:
      minio:
        condition: service_started
    deploy:
      resources:
        limits:
          memory: 512m
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8181/v1/config"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: [ "streaming" ]

  # ========================================
  # Investigations API
  # ========================================
  investigations-api:
    build:
      context: ./investigations-api
      dockerfile: Dockerfile
    image: dp_investigations_api:latest
    container_name: dp_investigations_api
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_started
    environment:
      ENV: development
      DATABASE_URL: postgresql+asyncpg://superset:superset@postgres:5432/superset
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: minio
      MINIO_SECRET_KEY: minio12345
      MINIO_BUCKET: investigations
      MINIO_USE_SSL: "false"
      LOG_LEVEL: INFO
    ports:
      - "8090:8000"
    volumes:
      - ./investigations-api:/app
    deploy:
      resources:
        limits:
          memory: 512m
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: [ "tiny", "standard" ]

  # Investigation UI - Simple web interface for managing investigations
  investigations-ui:
    image: nginx:alpine
    container_name: dp_investigations_ui
    ports:
      - "8080:80"
    volumes:
      - ./investigations-ui.html:/usr/share/nginx/html/index.html:ro
      - ./investigation-detail.html:/usr/share/nginx/html/investigation-detail.html:ro
      - ./dashboard.html:/usr/share/nginx/html/dashboard.html:ro
    networks:
      - default
    profiles: [ "tiny", "standard" ]

  keycloak:
    image: quay.io/keycloak/keycloak:23.0
    container_name: dp_keycloak
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # Database configuratie
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgres:5432/keycloak
      KC_DB_USERNAME: superset
      KC_DB_PASSWORD: superset
      
      # Admin credentials
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
      
      # Hostname configuratie
      KC_HOSTNAME: localhost
      KC_HOSTNAME_PORT: 8085
      KC_HOSTNAME_STRICT: false
      KC_HOSTNAME_STRICT_HTTPS: false
      
      # HTTP configuratie (voor development)
      KC_HTTP_ENABLED: true
      KC_HEALTH_ENABLED: true
      KC_METRICS_ENABLED: true
      
      # Proxy configuratie
      KC_PROXY: edge
    command:
      - start-dev
      - --import-realm
    ports:
      - "8085:8080"
    volumes:
      - keycloak_data:/opt/keycloak/data
    healthcheck:
      test: ["CMD-SHELL", "exec 3<>/dev/tcp/127.0.0.1/8080;echo -e 'GET /health/ready HTTP/1.1\r\nhost: http://localhost\r\nConnection: close\r\n\r\n' >&3;if [ $? -eq 0 ]; then echo 'Healthcheck Successful';exit 0;else echo 'Healthcheck Failed';exit 1;fi;"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 1.5g
    profiles: [ "standard" ]

volumes:
  pgdata:
  minio_data:
  konga_data:
  konga_pgdata:
  amundsen_neo4j:
  amundsen_elasticsearch:
  kafka_data:
  iceberg_catalog_data:
  keycloak_data:


