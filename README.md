# ğŸš€ Modern Data Platform

Complete modern data platform with streaming, lakehouse, visualization, and governance capabilities.

---

## ğŸ¯ Platform Overview

**End-to-end data platform featuring:**
- ğŸ“Š **Data Warehousing**: PostgreSQL with cell towers & crypto data  
- ğŸŒŠ **Streaming**: Kafka + Iceberg REST catalog for real-time crypto trades
- ğŸ¨ **Visualization**: Apache Superset for dashboards & analytics
- ğŸ”§ **Orchestration**: Dagster for workflow management
- ğŸ” **Query Engine**: DuckDB for fast SQL analytics
- ğŸ“¦ **Data Lake**: MinIO (S3-compatible) for Iceberg tables
- ğŸšª **API Gateway**: Kong + Konga for API management
- ğŸ“ˆ **Lineage**: Marquez for data lineage tracking
- ğŸ” **Catalog**: Amundsen for data discovery

---

## âš¡ Quick Start (5 minutes)

### 1. Start the Platform

```bash
# Option A: Minimal (Superset + PostgreSQL + MinIO + Dagster)
docker-compose --profile tiny up -d

# Option B: Full platform (all features except streaming)
docker-compose --profile standard up -d

# Option C: With streaming (includes Kafka + Iceberg)
docker-compose --profile streaming up -d

# Option D: With data catalog (includes Amundsen)
docker-compose --profile amundsen up -d
```

### 2. Initialize Platform

```bash
# Run bootstrap script to initialize all services
./scripts/bootstrap.sh
```

**This script will:**
- âœ… Wait for all services to be healthy
- âœ… Initialize Superset (DB migrations, admin user, permissions)
- âœ… Create MinIO bucket for data lake
- âœ… Sync crypto data to PostgreSQL (if available)

**First-time setup:** Takes ~2 minutes

### 3. Access Services

| Service | URL | Credentials | Purpose |
|---------|-----|-------------|---------|
| **Superset** | http://localhost:8088 | admin/admin | Dashboards & Visualization |
| **Dagster** | http://localhost:3000 | - | Workflow Orchestration |
| **MinIO Console** | http://localhost:9001 | minio/minio12345 | S3-compatible Data Lake |
| **Marquez UI** | http://localhost:3001 | - | Data Lineage |
| **Konga** | http://localhost:1337 | Setup on first run | API Gateway UI |
| **Amundsen** | http://localhost:5005 | - | Data Catalog |

---

## ğŸ“Š Available Datasets

### Cell Towers Dataset
- **Source**: OpenCellID via Google Cloud Storage
- **Records**: 47,114 cell tower locations
- **Schema**: `cell_towers.clean_204`
- **Columns**: mcc, net, area, cell, unit, lon, lat, range, samples, changeable, created, updated, averageSignal

### Crypto Streaming Dataset (Optional)
- **Source**: Binance WebSocket â†’ Kafka â†’ Iceberg
- **Bronze Layer**: `crypto.trades_bronze` (raw trades)
- **Silver Layer**: `crypto.trades_1min` (OHLCV candles aggregated per minute)
- **Symbols**: ETHUSDT, BNBUSDT
- **Setup**: See [Crypto Stream Quickstart](docs/CRYPTO_STREAM_QUICKSTART.md)

---

## ğŸš€ Common Workflows

### Load Cell Towers Data (One-time)

```bash
# Run ETL job to load cell towers from GCS
docker-compose up etl
```

**What it does:**
1. Downloads 204.csv from Google Cloud Storage
2. Loads to staging table in PostgreSQL
3. Cleans and validates data
4. Creates final table `cell_towers.clean_204`
5. Registers metadata in Marquez

### Start Crypto Streaming Pipeline

```bash
# 1. Start streaming services
docker-compose --profile streaming up -d

# 2. Materialize bronze layer (consumes Kafka â†’ Iceberg)
docker-compose exec dagster dagster asset materialize -m crypto_stream --select crypto_trades_bronze

# 3. Materialize silver layer (aggregates to 1-minute candles)
docker-compose exec dagster dagster asset materialize -m crypto_stream --select crypto_trades_1min

# 4. Sync to PostgreSQL for Superset
./scripts/bootstrap.sh
```

### Query Data in Superset

```bash
# 1. Open Superset
open http://localhost:8088

# 2. Login: admin / admin

# 3. SQL Lab â†’ Database: PostgreSQL - Data Platform

# 4. Run queries:
# Cell towers by country
SELECT mcc, COUNT(*) as tower_count
FROM cell_towers.clean_204
GROUP BY mcc
ORDER BY tower_count DESC
LIMIT 10;

# Latest crypto prices
SELECT * FROM crypto.latest_prices;

# OHLCV candles
SELECT * FROM crypto.trades_1min
ORDER BY minute DESC
LIMIT 20;
```

---

## ğŸ“š Documentation

### ğŸ“ Getting Started
- ğŸš€ **[Quick Reference](docs/QUICK_REFERENCE.md)** - Essential commands & workflows
- ğŸ‘¨â€ğŸ’» **[Data Engineer Onboarding](docs/DATA_ENGINEER_ONBOARDING.md)** - Complete onboarding guide
- ğŸ“Š **[Platform Status](docs/PLATFORM_STATUS.md)** - Current capabilities

### ğŸ’° Crypto Streaming
- ğŸ—ï¸ **[Crypto Stream Architecture](docs/CRYPTO_STREAM_ARCHITECTURE.md)** - System design
- âš¡ **[Crypto Stream Quickstart](docs/CRYPTO_STREAM_QUICKSTART.md)** - Get started guide

### ğŸ“Š Superset & Visualization
- ğŸ”Œ **[Superset Database Connections](docs/SUPERSET_DATABASE_CONNECTIONS.md)** - PostgreSQL & DuckDB setup
- âœ… **[Superset Problem Solved](docs/SUPERSET_PROBLEM_SOLVED.md)** - DuckDB workarounds
- ğŸ¦† **[DuckDB Superset Guide](docs/DUCKDB_SUPERSET_GUIDE.md)** - Fast analytics on Parquet/Iceberg
- ğŸ“‹ **[Superset Quick Reference](docs/SUPERSET_QUICK_REFERENCE.md)** - Connection strings & queries

### ğŸ”§ Platform Components
- ğŸŒ¤ï¸ **[Weather API](docs/WEATHER_API.md)** - RESTful weather data API (bonus feature)
- ğŸ”§ **[Dagster gRPC Fix](docs/DAGSTER_GRPC_FIX.md)** - Troubleshooting guide
- ğŸ“ˆ **[Marquez Integration](docs/MARQUEZ_INTEGRATION.md)** - Data lineage setup

---

## ğŸ—ï¸ Architecture

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Sources   â”‚
â”‚  - GCS (CSV)    â”‚
â”‚  - Binance WS   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ingestion      â”‚
â”‚  - ETL Jobs     â”‚
â”‚  - Kafka        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Storage        â”‚
â”‚  - PostgreSQL   â”‚
â”‚  - MinIO/Icebergâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Layer    â”‚
â”‚  - DuckDB       â”‚
â”‚  - PostgreSQL   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Presentation   â”‚
â”‚  - Superset     â”‚
â”‚  - APIs         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Dependencies

```
PostgreSQL (Core Database)
    â”œâ”€â”€ Superset (requires PostgreSQL)
    â”œâ”€â”€ Dagster (metadata storage)
    â”œâ”€â”€ Marquez (lineage storage)
    â””â”€â”€ ETL jobs (data warehouse)

MinIO (Data Lake)
    â”œâ”€â”€ Iceberg REST Catalog
    â””â”€â”€ Dagster (Iceberg I/O)

Kafka (Streaming)
    â””â”€â”€ Dagster crypto_stream (consumer)
```

---

## ğŸ§¹ Maintenance

### Stop & Clean

```bash
# Stop all services
docker-compose down

# Stop and remove volumes (DELETES ALL DATA!)
docker-compose down -v

# Clean up unused images
docker system prune -a
```

### Restart from Scratch

```bash
# 1. Stop everything and remove volumes
docker-compose down -v

# 2. Start services
docker-compose --profile standard up -d

# 3. Run bootstrap
./scripts/bootstrap.sh

# 4. Load data
docker-compose up etl

# Done! Everything is fresh and initialized
```

### Backup Important Data

```bash
# Backup PostgreSQL
docker-compose exec postgres pg_dump -U superset superset > backup_$(date +%Y%m%d).sql

# Backup MinIO bucket
docker-compose exec dagster python3 -c "
from minio import Minio
client = Minio('minio:9000', access_key='minio', secret_key='minio12345', secure=False)
# Use minio client to download bucket contents
"

# Restore PostgreSQL
cat backup_20251012.sql | docker-compose exec -T postgres psql -U superset -d superset
```

---

## ğŸ› Troubleshooting

### Service won't start

```bash
# Check service status
docker-compose ps

# View logs
docker-compose logs -f <service_name>

# Restart specific service
docker-compose restart <service_name>
```

### Superset can't connect to database

```bash
# Re-initialize Superset
docker-compose exec superset superset db upgrade
docker-compose exec superset superset init
docker-compose restart superset
```

### Dagster assets won't materialize

```bash
# Check Dagster logs
docker-compose logs -f dagster

# Verify MinIO is accessible
docker-compose exec dagster curl http://minio:9000/minio/health/live

# Check Kafka (if using streaming)
docker-compose exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092
```

### Out of memory errors

```bash
# Check resource usage
docker stats

# Increase Docker Desktop memory (Preferences â†’ Resources â†’ Memory)
# Recommended: 8GB minimum for full platform
```

---

## ğŸ¤ Contributing

### Project Structure

```
data-platform/
â”œâ”€â”€ docker-compose.yml          # Service orchestration
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ bootstrap.sh           # Platform initialization
â”œâ”€â”€ postgres-init/             # PostgreSQL init scripts
â”‚   â”œâ”€â”€ 01_init_marquez.sql
â”‚   â”œâ”€â”€ 02_init_weather.sql
â”‚   â””â”€â”€ 03_init_crypto.sql
â”œâ”€â”€ orchestration/             # Dagster workflows
â”‚   â”œâ”€â”€ crypto_stream/         # Crypto streaming pipeline
â”‚   â””â”€â”€ weather_pipeline/      # Weather data pipeline
â”œâ”€â”€ superset/                  # Superset configuration
â”œâ”€â”€ etl/                       # Cell towers ETL
â”œâ”€â”€ api/                       # RESTful APIs
â””â”€â”€ docs/                      # Documentation
```

### Development Workflow

1. Make changes to service configuration
2. Test with `docker-compose up -d`
3. Run `./scripts/bootstrap.sh` to verify initialization
4. Update documentation in `docs/`
5. Commit and push

---

## ğŸ“ License

This project is for educational and demonstration purposes.

---

## ğŸ‰ What's Next?

1. âœ… **Load cell towers data**: `docker-compose up etl`
2. âœ… **Create your first Superset dashboard** (see [docs/SUPERSET_DATABASE_CONNECTIONS.md](docs/SUPERSET_DATABASE_CONNECTIONS.md))
3. âœ… **Start crypto streaming** (see [docs/CRYPTO_STREAM_QUICKSTART.md](docs/CRYPTO_STREAM_QUICKSTART.md))
4. âœ… **Explore data lineage** in Marquez: http://localhost:3001
5. âœ… **Query with DuckDB**: Fast analytics on Iceberg tables

**Questions?** Check the [docs/](docs/) folder for detailed guides!
